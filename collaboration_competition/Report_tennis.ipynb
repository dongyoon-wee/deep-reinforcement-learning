{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, DDPG multi-agents are trained on Tennis environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent, ReplayBuffer\n",
    "from train_agent import train_agent\n",
    "from infer_agent import infer_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, DDPG(Deep Deterministic Policy Gradients) is modified to apply for multi agents task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "DDPG is working quite well on continuous action space such as Reacher environment. The significant difference from Reacher environment to Tennis environment is the number of agents. While Reacher environment provides an observation for a single agent, Tennis environment provides an observation for two each agents. Thus, in this project, DDPG is modified to apply for multi agents environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyper parameters to configure\n",
    "\n",
    "* RANDOM_SEED : random seed\n",
    "* BUFFER_SIZE : replay buffer size\n",
    "* BATCH_SIZE : minibatch size\n",
    "* GAMMA : discount factor\n",
    "* TAU : for soft update of target parameters\n",
    "* LR_ACTOR : learning rate of the actor\n",
    "* LR_CRITIC : learning rate of the critic\n",
    "* EIGHT_DECAY : L2 weight decay\n",
    "* NUM_EPISODES : number of episodes to train\n",
    "* MAX_T : maximum number of iterations to train per episode\n",
    "* SUCCESS_SCORE : success score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "BUFFER_SIZE = int(1e6)  \n",
    "BATCH_SIZE = 128        \n",
    "GAMMA = 0.999            \n",
    "TAU = 1e-3              \n",
    "LR_ACTOR = 1e-4         \n",
    "LR_CRITIC = 1e-4        \n",
    "WEIGHT_DECAY = 0.0001        \n",
    "NUM_EPISODES = 10000     \n",
    "MAX_T = 100            \n",
    "SUCCESS_SCORE = 0.5      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = Agent(state_size, action_size, memory, random_seed=RANDOM_SEED, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "agent2 = Agent(state_size, action_size, memory, random_seed=RANDOM_SEED, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000\tAverage Score: 0.00\tAverage Time: 5.8907351493835454==4==\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGPxJREFUeJzt3X2UXXV97/H3x4wJT8pDEilNghNvom1cuBRGxFXp5ZoKob0l19uwSOAuo6VFW9Hba7tqqHchcHvvKixLqtdYCIKlUUkoWpslwYjGCz5AzCAIBBgYApJEMZMHAuEpT9/7x/4NHE4ns88k85tzzj6f11qzsvdv/8453z17Mp/ZT7+tiMDMzGw4r2t2AWZm1vocFmZmVsphYWZmpRwWZmZWymFhZmalHBZmZlbKYWFmZqUcFmZmVsphYWZmpbqaXcBomTRpUnR3dze7DDOztnLPPfdsjYjJZf0qExbd3d309vY2uwwzs7Yi6ReN9PNhKDMzK+WwMDOzUg4LMzMr5bAwM7NSDgszMyuVNSwkzZHUJ6lf0qIhlv+upJ9J2itpXt2yhZIeS18Lc9ZpZmbDyxYWksYBS4CzgVnAAkmz6ro9BXwY+Hrda48DPgu8BzgV+KykY3PVamZmw8u5Z3Eq0B8RGyJiN7AcmFvbISKejIj7gf11rz0LuD0itkfEDuB2YE7GWrPZtz/4y5t/zqO/fu6Q3ucnj2/lrMV3snH7C6NUmY2Gux7fxuMDu5pdRjb9W57j7g3bml2GtYCcYTEF2Fgzvym1jdprJV0kqVdS78DAwEEXmtOyu57kGz/bxJmL7zyk9zn/urX0/fo5Tr/qB6NTmI2KBdfdzey/v6PZZWTze1ffyfyldze7DGsBbX2COyKWRkRPRPRMnlx6t3pT7HhhT7NLMDM7ZDnDYjMwrWZ+amrL/dqWIjW7AjOzQ5czLNYBMyVNlzQemA+sbPC1q4EzJR2bTmyfmdrajnBamFn7yxYWEbEXuJjil/zDwM0RsV7SFZLOAZD0bkmbgHOBayWtT6/dDvwvisBZB1yR2tqO9yzMrAqyjjobEauAVXVtl9ZMr6M4xDTUa28AbshZ31hwVphZFbT1Ce524D0LM6sCh0VmclqYWQU4LDJzVphZFTgsMvPVUGZWBQ6LzLxnYWZV4LDIzFlhZlXgsMjMexZmVgUOi8xe57QwswpwWJiZWSmHRWa+z8LMqsBhkZmjwsyqwGGRmXcszKwKHBaZOSvMrAocFpn5nEXriQj+z6qHeeTpZxvq+3e3PcL6X+4cg8oO7Imtz3PZyvXs3x9NraPM/+vbQveiW/nYsnvYWfeUyJ/0b+WaOx5vUmV2qBwWme14YXezS7A625/fzdI7N3D+dWtL+764Zx/X3PE48/7xrjGo7MA+uqyXf/rJkzy2ZVdT6yjz4a+sA+A765/m/6557DXLzv/yWv7utkeaUZaNAoeFWRuItEPhHVVrFoeFmZmVcliYmVkph0Vm0drnI83MGuKwMDOzUg4LMzMr5bAwM7NSDovMfKmjmVWBw8LMzEo5LMzMrJTDwjpWjOC65qA1roFup0ux26hUa4DDIjN53NmWM5LBHVtl+/nclzWbw8LMzEo5LMzMrJTDwszMSmUNC0lzJPVJ6pe0aIjlEyStSMvXSupO7a+XdKOkByQ9LOmSnHXm5GPNZlYF2cJC0jhgCXA2MAtYIGlWXbcLgR0RMQNYDFyZ2s8FJkTEScApwEcHg8TMzMZezj2LU4H+iNgQEbuB5cDcuj5zgRvT9C3AbBWXqgRwpKQu4HBgN1D+DEwzM8siZ1hMATbWzG9KbUP2iYi9wE5gIkVwPA/8CngK+FxEbM9Yq5mZDaNVT3CfCuwDfhOYDvylpLfUd5J0kaReSb0DAwNjXaOZWcfIGRabgWk181NT25B90iGno4FtwPnAdyJiT0RsAX4M9NR/QEQsjYieiOiZPHlyhlUwMzPIGxbrgJmSpksaD8wHVtb1WQksTNPzgDVRjMHwFPB+AElHAqcBj2Ss1TqIh/kwG7lsYZHOQVwMrAYeBm6OiPWSrpB0Tup2PTBRUj/wKWDw8tolwFGS1lOEzlci4v5ctZqZ2fC6cr55RKwCVtW1XVoz/RLFZbL1r9s1VHs78m0WrcdjQ5mNXKue4DYzsxbisDAzs1IOCzMzK+WwMDOzUg6LzHzFo5lVgcPCzMxKOSwy8xWPZlYFDgszMyvlsDCzLDxESbU4LDLznbdmVgUOC+tYI/nDt1X+Sm6VgQ2t8zgsrOOMZGevVfYMW2WMKutcDgszMyvlsDAzs1IOi8xGMhy2mVmrcliYmVkph4WZmZVyWJiZWSmHhZmZlXJYWMfxzXhmI+ewsI7lm/PMGuewMDOzUg4L61g+HGXWOIeFdRwffjIbOYeFmZmVcliYmVkph4WZmZVyWJiZWSmHhZmZlXJYmJlZKYeFmWXhe0KqJWtYSJojqU9Sv6RFQyyfIGlFWr5WUnfNsndIukvSekkPSDosZ625tMp1+mZmhyJbWEgaBywBzgZmAQskzarrdiGwIyJmAIuBK9Nru4CvAh+LiLcDZwB7ctVqZmbDy7lncSrQHxEbImI3sByYW9dnLnBjmr4FmK3iOaRnAvdHxM8BImJbROzLWKuZmQ0jZ1hMATbWzG9KbUP2iYi9wE5gIvBWICStlvQzSX891AdIukhSr6TegYGBUV8Bq7aRjPfUKkffW2WMKus8rXqCuwt4H3BB+veDkmbXd4qIpRHRExE9kydPHusarU2143mkdqzZqiVnWGwGptXMT01tQ/ZJ5ymOBrZR7IXcGRFbI+IFYBVwcsZazcxsGDnDYh0wU9J0SeOB+cDKuj4rgYVpeh6wJiICWA2cJOmIFCL/EXgoY63ZeLRQM6uCrlxvHBF7JV1M8Yt/HHBDRKyXdAXQGxErgeuBZZL6ge0UgUJE7JB0NUXgBLAqIm7NVauZmQ0vW1gARMQqikNItW2X1ky/BJx7gNd+leLyWTMza7JWPcFtZmYtxGFhHacdLz9tx5qtWhoOC0nvk/SRND1Z0vR8ZZnlN5LLUVvlMgVfQmvN0lBYSPos8GngktT0enw+wcysYzS6Z/FB4BzgeYCI+CXwhlxFmZlZa2k0LHan+x8CQNKR+UqqFh82aF0e7sOscY2Gxc2SrgWOkfSnwPeA6/KVZWZmraSh+ywi4nOSPgA8C7wNuDQibs9amVkm7bi31441W7WUhkV6LsX3IuI/AQ4IM7MOVHoYKj1HYr+ko8egHjMza0GNDvexC3hA0u2kK6IAIuKTWaoyM7OW0mhYfDN9mZlZB2r0BPeNaZjxt6amvojwM7Eb4POS1ql8mW+1NBQWks6geFb2kxS//6ZJWhgRd+YrzczMWkWjh6H+HjgzIvoAJL0VuAk4JVdhZmbWOhq9Ke/1g0EBEBGPUowPZWZmHaDRPYteSV/m1cEDLwB685RkZmatptGw+DPg48DgpbI/BL6UpSKzMRJtODiUTxpbszQaFl3A5yPianjlru4J2aoyM7OW0ug5i+8Dh9fMH04xmKBZ25EvaDYbsUbD4rCI2DU4k6aPyFOSmZm1mkbD4nlJJw/OSOoBXsxTUrV4tFAzq4JGz1n8BfAvkn6Z5k8AzstTkpmZtZph9ywkvVvSb0TEOuC3gBXAHuA7wBNjUJ/ZqIsRXNrkq4/MCmWHoa4Fdqfp9wJ/AywBdgBLM9Zllp1GcoywRQ4n+rCmNUvZYahxEbE9TZ8HLI2IbwDfkHRf3tLMzKxVlO1ZjJM0GCizgTU1yxo932FmZm2u7Bf+TcAdkrZSXP30QwBJM4CdmWszM7MWMWxYRMT/lvR9iqufvhuvjo/wOuATuYurAt8AZmZVUHooKSLuHqLt0TzlmI0djw1l1rhGb8o7KJLmSOqT1C9p0RDLJ0hakZavldRdt/xESbsk/VXOOq2zeG/PbOSyhUUabHAJcDYwC1ggaVZdtwuBHRExA1gMXFm3/Grgtlw1mplZY3LuWZwK9EfEhojYDSwH5tb1mUvxuFaAW4DZShe/S/ovFDf+rc9Yo5mZNSBnWEwBNtbMb0ptQ/aJiL0UV1hNlHQU8Gng8oz1mZlZg7KeszgElwGLa0e6HYqkiyT1SuodGBgYm8rMzDpQzhvrNgPTauanprah+mxKN/8dDWwD3gPMk3QVcAywX9JLEfHF2hdHxFLSsCM9PT0teZ2Ih2cwsyrIGRbrgJmSplOEwnzg/Lo+K4GFwF3APGBNupfj9MEOki4DdtUHhZmZjZ1sYREReyVdDKwGxgE3RMR6SVcAvRGxErgeWCapH9hOEShmZtZiso7vFBGrgFV1bZfWTL8EnFvyHpdlKc7MzBrWqie4zcyshTgszMyslMPCOtZILp8bydP1cmqVOqzzOCys87Th5cwjeqqfWQYOCzMzK+WwMDOzUg4L6zwjeoxFa5wjGNGzN1pEO9ZsB+awMDOzUg4L61gjOWXcKg9MapU6GuGT8tXisDAzs1IOCzMzK+WwMDOzUg4LMzMr5bDIzCf5zKwKHBbWsTw2lFnjHBbWedpwZ897qNZsDgszMyvlsDCzLDzcR7U4LMzMrJTDwszMSjksMvNpSTOrAoeFmZmVcliYmVkph0Vmvh7EzKrAYWFmZqUcFta5RvJ41RbZRWyVOqzzOCzMzKyUw8KsDfgSbGs2h0Vm/k/egtrx8FOzCzgI7VizHZjDIjP/hzGzKnBYWOcawW5fq4wQ3ip1NKKNSrUGZA0LSXMk9Unql7RoiOUTJK1Iy9dK6k7tH5B0j6QH0r/vz1mnmZkNL1tYSBoHLAHOBmYBCyTNqut2IbAjImYAi4ErU/tW4A8j4iRgIbAsV51mZlYu557FqUB/RGyIiN3AcmBuXZ+5wI1p+hZgtiRFxL0R8cvUvh44XNKEjLWamdkwcobFFGBjzfym1DZkn4jYC+wEJtb1+SPgZxHxcv0HSLpIUq+k3oGBgVEr3MzMXqulT3BLejvFoamPDrU8IpZGRE9E9EyePHlsizMz6yA5w2IzMK1mfmpqG7KPpC7gaGBbmp8K/CvwoYh4PGOdWfmKEDOrgpxhsQ6YKWm6pPHAfGBlXZ+VFCewAeYBayIiJB0D3AosiogfZ6zROlk73pzXInVY58kWFukcxMXAauBh4OaIWC/pCknnpG7XAxMl9QOfAgYvr70YmAFcKum+9PWmXLWamdnwunK+eUSsAlbVtV1aM/0ScO4Qr/tb4G9z1mbWTtrxcKZ3gqqlpU9wm5lZa3BYmJlZKYeFmZmVclhk1k4Dv5mZHYjDwszMSjkszMyslMPCzMxKOSzMzKyUwyIzD89gZlXgsLCONZIcd+Zbp3NYmJlZKYeFdZwYwX6C9yjMCg4L61gjuV/S91Zap3NYmJlZKYeFmZmVcliYmVkph4WZmZVyWJiZWSmHhZmZlXJYZObnWZhZFTgszMyslMPCOpbHhjJrnMPCzLLwiMvV4rAwawM+92XN5rAwM7NSDgszMyvlsDAzs1IOi8x8qNnMqsBhYWZmpRwWZmZWKmtYSJojqU9Sv6RFQyyfIGlFWr5WUnfNsktSe5+ks3LWaWZmw8sWFpLGAUuAs4FZwAJJs+q6XQjsiIgZwGLgyvTaWcB84O3AHOBL6f3MzKwJcu5ZnAr0R8SGiNgNLAfm1vWZC9yYpm8BZktSal8eES9HxBNAf3o/MzNrgq6M7z0F2Fgzvwl4z4H6RMReSTuBian97rrXTslR5CNPP8snvn5vjrcG4LEtu16Z/sDVd4zKe47W+3SqffuLcSh2vby39Hu5L41ZsW9/DNs39zYZ/Dn68Fd+yrFHjM/6WUM5mPVbdvcvuHvDtlF5LxveGW+bzGf+oP7AzejKGRbZSboIuAjgxBNPPKj3OKxrHDOPP2o0y3qNGW86itsefBrgkD5n8JfFlGMOz1pvp9iw9Xl++4Q3Mn3SEeV9B57nbce/gf/wpiP/3bLHtuxi0lHjs2+Tow7r4t6nnuHd3ceN6dAfz7y4h4HnXm54/V7YvY/Nz7wIwOkzJ/GGw179FfPinn1s2vGif34zOP6Nh2X/jJxhsRmYVjM/NbUN1WeTpC7gaGBbg68lIpYCSwF6enoOatiy7klH8qULTjmYl5qZdYyc5yzWATMlTZc0nuKE9cq6PiuBhWl6HrAmIiK1z09XS00HZgI/zVirmZkNI9ueRToHcTGwGhgH3BAR6yVdAfRGxErgemCZpH5gO0WgkPrdDDwE7AU+HhH7ctVqZmbDU1Rk0Pmenp7o7e1tdhlmZm1F0j0R0VPWz3dwm5lZKYeFmZmVcliYmVkph4WZmZVyWJiZWanKXA0laQD4xSG8xSRg6yiV0w46bX3B69wpvM4j8+aImFzWqTJhcagk9TZy+VhVdNr6gte5U3id8/BhKDMzK+WwMDOzUg6LVy1tdgFjrNPWF7zOncLrnIHPWZiZWSnvWZiZWamODwtJcyT1SeqXtKjZ9RwKSdMk/UDSQ5LWS/rvqf04SbdLeiz9e2xql6QvpHW/X9LJNe+1MPV/TNLCA31mK5A0TtK9kr6d5qdLWpvWa0UaIp805P2K1L5WUnfNe1yS2vskndWcNWmMpGMk3SLpEUkPS3pvB2zj/5F+ph+UdJOkw6q2nSXdIGmLpAdr2kZtu0o6RdID6TVfkEb4GK2I6NgviqHTHwfeAowHfg7ManZdh7A+JwAnp+k3AI8Cs4CrgEWpfRFwZZr+feA2QMBpwNrUfhywIf17bJo+ttnrN8x6fwr4OvDtNH8zMD9NXwP8WZr+c+CaND0fWJGmZ6VtPwGYnn4mxjV7vYZZ3xuBP0nT44FjqryNKR6p/ARweM32/XDVtjPwu8DJwIM1baO2XSmeCXRaes1twNkjqq/Z36Amb5z3Aqtr5i8BLml2XaO4fv8GfADoA05IbScAfWn6WmBBTf++tHwBcG1N+2v6tdIXxVMUvw+8H/h2+o+wFeiq38YUz1Z5b5ruSv1Uv91r+7XaF8XTJJ8gnW+s33YV3cZTgI3pF2BX2s5nVXE7A911YTEq2zUte6Sm/TX9Gvnq9MNQgz+EgzaltraXdr3fBawFjo+IX6VFTwPHp+kDrX87fV/+AfhrYH+anwg8ExF703xt7a+sV1q+M/Vvp/WdDgwAX0mH3r4s6UgqvI0jYjPwOeAp4FcU2+0eqr2dB43Wdp2SpuvbG9bpYVFJko4CvgH8RUQ8W7ssij8rKnEJnKT/DGyJiHuaXcsY6qI4VPGPEfEu4HmKwxOvqNI2BkjH6edSBOVvAkcCc5paVBM0e7t2elhsBqbVzE9NbW1L0uspguJrEfHN1PxrSSek5ScAW1L7gda/Xb4vvwOcI+lJYDnFoajPA8dIGnxkcG3tr6xXWn40sI32WV8o/iLcFBFr0/wtFOFR1W0M8HvAExExEBF7gG9SbPsqb+dBo7VdN6fp+vaGdXpYrANmpqsqxlOcDFvZ5JoOWrq64Xrg4Yi4umbRSmDwqoiFFOcyBts/lK6sOA3YmXZ5VwNnSjo2/VV3ZmprKRFxSURMjYhuim23JiIuAH4AzEvd6td38PswL/WP1D4/XUUzHZhJcTKw5UTE08BGSW9LTbMpnlVfyW2cPAWcJumI9DM+uM6V3c41RmW7pmXPSjotfQ8/VPNejWn2CZ1mf1FcVfAoxZURn2l2PYe4Lu+j2E29H7gvff0+xfHa7wOPAd8Djkv9BSxJ6/4A0FPzXn8M9KevjzR73RpY9zN49Wqot1D8EugH/gWYkNoPS/P9aflbal7/mfR96GOEV4k0YV3fCfSm7fwtiqteKr2NgcuBR4AHgWUUVzRVajsDN1Gck9lDsQd54WhuV6Anff8eB75I3UUSZV++g9vMzEp1+mEoMzNrgMPCzMxKOSzMzKyUw8LMzEo5LMzMrJTDwjqepH2S7qv5Gnb0YUkfk/ShUfjcJyVNOojXnSXp8jQi6W2HWodZI7rKu5hV3osR8c5GO0fENTmLacDpFDeknQ78qMm1WIfwnoXZAaS//K9KzwD4qaQZqf0ySX+Vpj+p4vkh90tantqOk/St1Ha3pHek9omSvqviuQxfprixavCz/lv6jPskXStp3BD1nCfpPuCTFAMoXgd8RFLbjjpg7cNhYQaH1x2GOq9m2c6IOInijtd/GOK1i4B3RcQ7gI+ltsuBe1Pb3wD/nNo/C/woIt4O/CtwIoCk3wbOA34n7eHsAy6o/6CIWEExkvCDqaYH0mefcygrb9YIH4YyG/4w1E01/y4eYvn9wNckfYti6A0ohl35I4CIWJP2KN5I8XCb/5rab5W0I/WfDZwCrEsPLzucVweMq/dWigfaABwZEc81sH5mh8xhYTa8OMD0oD+gCIE/BD4j6aSD+AwBN0bEJcN2knqBSUCXpIeAE9JhqU9ExA8P4nPNGubDUGbDO6/m37tqF0h6HTAtIn4AfJpiKOyjgB+SDiNJOgPYGsVzRe4Ezk/tZ1MMAAjFQHHzJL0pLTtO0pvrC4mIHuBWimc7XEUx8OU7HRQ2FrxnYZbOWdTMfyciBi+fPVbS/cDLFI+irDUO+Kqkoyn2Dr4QEc9Iugy4Ib3uBV4dYvpy4CZJ64GfUAy9TUQ8JOl/At9NAbQH+DjwiyFqPZniBPefA1cPsdwsC486a3YA6aFKPRGxtdm1mDWbD0OZmVkp71mYmVkp71mYmVkph4WZmZVyWJiZWSmHhZmZlXJYmJlZKYeFmZmV+v8v+8ICVPDdSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = None\n",
    "scores = train_agent(env, agent1, agent2, brain_name, n_episodes=NUM_EPISODES, success_score=SUCCESS_SCORE, list_scores=scores)\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Infer agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'success'\n",
    "model_path = 'models/{0}_{1}.pth'\n",
    "actor1_model_path = model_path.format('actor1', tag)\n",
    "critic1_model_path = model_path.format('critic1', tag)\n",
    "actor2_model_path = model_path.format('actor2', tag)\n",
    "critic2_model_path = model_path.format('critic2', tag)\n",
    "memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, RANDOM_SEED)\n",
    "\n",
    "if os.path.isfile(actor_model_path) & os.path.isfile(critic_model_path):\n",
    "    agent1.load_actor(actor1_model_path)\n",
    "    agent1.load_critic(critic1_model_path)\n",
    "    agent2.load_actor(actor2_model_path)\n",
    "    agent2.load_critic(critic2_model_path)\n",
    "    print('Complete to load models from {0} | {1}'.format(actor_model_path, critic_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = infer_agent(env, agent1, agent2, memory, brain_name, n_episodes=100)\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Future works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current agent fail to solve the task. It seems that agents cannot learn policy and value network properly from experiences. As first future work hyper parameter tuning is planned. Besides for second future work, Actor-Critic algorithm such as A3C, A2C is another approach in future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
